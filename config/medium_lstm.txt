finetuning	False
sequence_model_name	lstm
vocab_size	30012
batchsize	512
num_layers	3
num_stacked_lstm	2
mlp_dropout	0
seed	0,1,2,3,4
num_epochs	50000
weight_decay	0
use_for_patience	acc
early_stop	15
max_seq_len	200
transformer_nhead	1
dropout	0.2
eval_every	1
learning_rate	0.0001
mlp_hidden_dim	128
batch_size	128
device	cuda
hidden_dim	64
predefined_vocab	vocab/ksyn5_5000_vocab.txt
pretraining	True
char_embedding_dim	32
transformer_stackedlayers	1
min_vocab_frequency	1
test_directory	data/programming/
experiment_label	acl-proplogic-ksyn5-medium-lstm
pretraining_dir	data/pcfg/ksyn_{5}_syntactic/data,data/pcfg/ksyn_{5}_truthfulness/data,data/pcfg/ksyn_{5}_informativity2T2A/data,data/pcfg/ksyn_{5}_grounding/data
write_dir	output/acl-proplogic-ksyn5-medium-lstm
DANGER_all_embeddings_same	False
experiment_folder	acl-proplogic-ksyn5-medium-lstm
