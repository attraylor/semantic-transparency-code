finetuning	False
sequence_model_name	transformer
vocab_size	30012
batchsize	512
num_layers	3
num_stacked_lstm	3
mlp_dropout	0
seed	0,1,2,3,4
num_epochs	50000
weight_decay	0
use_for_patience	acc
early_stop	15
max_seq_len	200
transformer_nhead	4
dropout	0.2
eval_every	1
learning_rate	5e-05
mlp_hidden_dim	128
batch_size	128
device	cuda
pretraining	True
char_embedding_dim	32
transformer_stackedlayers	4
min_vocab_frequency	1
predefined_vocab	vocab/ksyn5_5000_vocab.txt
hidden_dim	128
test_directory	data/programming/
experiment_label	acl-proplogic-ksyn5-medium-transformer
pretraining_dir	data/pcfg/ksyn_{5}_syntactic/data,data/pcfg/ksyn_{5}_truthfulness/data,data/pcfg/ksyn_{5}_informativity2T2A/data,data/pcfg/ksyn_{5}_grounding/data
write_dir	output/acl-proplogic-ksyn5-medium-transformer
DANGER_all_embeddings_same	False
experiment_folder	acl-proplogic-ksyn5-medium-transformer
